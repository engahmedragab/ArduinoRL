\documentclass{article}

\usepackage{graphicx} % Required for the inclusion of images
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage[final]{pdfpages}
\usepackage[parfill]{parskip}
\usepackage{bm}

\usepackage[utf8]{inputenc}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage{mathtools} 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{caption}

\usepackage{amsmath,amsfonts,amssymb}

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

\newcommand{\grad}[1]{\nabla_{#1}}
\newcommand{\gradv}[1]{\nabla_{\bm{#1}}}
\newcommand{\thetab}{\bm{\theta}}
\newcommand{\phib}{\bm{\phi}}

\long\def\/*#1*/{}

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Embedded Agent for Execution-efficient Power-optimal Joint Control}

\author{\textsc{Nick Walker}}

\date{CS394R Fall 2016} % Date for the report

\begin{document}
	
\maketitle % Insert the title, author and date

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Motivation}

Embedded agents, which face memory and compute constraints, have applications in many areas, including space and consumer electronics. In a previous programming assignment, I solved a simple task on a memory-constrained two-joint arm system using a temporal difference learning approach, which was possible because even a coarse representation of the state and action spaces was sufficient to achieve good performance. In many problems however, we would like to achieve higher, near-optimal levels of performance. In this report, I investigate a complex joint control task, one such problem where near-optimal behavior is highly desirable and examine how a learning approach might be applied.


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Introduction}

\subsection{Learning Platform}

I based the platform on the Teensy 3.2, an ARM Cortex-M4 development board. Compared to the popular AVR microcontrollers, ARM chips are substantially more powerful, and not much more expensive. Though both chips lack a floating point unit, ARM cores have an emulation advantage due to the their higher clock speed and native 32-bit operation capabilities. The Cortex line has seen substantial commercial use, so evaluating an embedded agent using the M4 provides useful insight into the constraints of real systems are.

\begin{center}
	\begin{tabular}{ l l l}
		& AtMega328p & Cortex-M4  \\ \midrule
		Instruction set & AVR & ARMv7-M\\
		Integer width & 8 bits & 32 bits\\
		SRAM & 2kb & 32kb\\
		Program memory & 32kb & 256kb\\
		Clock Speed & 16Mhz & 72Mhz\\ 
		FPU & No & No
		
	\end{tabular}
\end{center}

In order to provide a challenging task for the agent, I built a six degree of freedom arm. The arm's joints are MTR955 servos, each capable of 170$\degree$ of rotation, though some joints have less range of motion to physical constraints.

\begin{center}
	\begin{tabular}{ l l l}
		Joint & Rotation range ($\deg$)  \\ \midrule
		0 & 160 \\
		1 & 120 \\
		2 & 50 \\
		3 & 50 \\
		4 & 50\\ 
		5 & 30
		
	\end{tabular}
\end{center}

\begin{figure}
	\centering
	%\includegraphics[width=10cm]{../photos/eagle_small.jpg}
	%\includegraphics[width=10cm]{../photos/arm_detail_small.jpg}
	\caption{Top: The learning platform. Five joints control the articulation of the arm, while the sixth opens and closes the grasper.}
	\label{fig:platform}
\end{figure}


\subsection{Problem}

Motors consume a substantial amount of power, so it is important for a battery-powered mobile system to have an efficient motor usage policy. There are two components to an efficient motor policy. \textit{Utilization efficiency}, making movements only as necessary, and \textit{execution efficiency}, using joints to actuate movements as efficiently as possible. Speed is not the only component of execution efficiency. Motors consume power proportional to the torque they apply, so in some systems, there may ways of using properties of the environment to transition between joint poses that require less torque. In the arm system for instance, a policy that moves in such a way that gravity pulls joints towards the target configuration may be more power efficient, even if it takes longer than a direct transition. With a good model of the system, it is possible to engineer a power-optimal execution policy, however such models may be expensive or impossible to obtain. Further, because the model will be imperfect, small differences, like the precise angles of the joints or the quality of the motors may change the optimal policy. For these reasons, a learning approach, which can adapt to the idiosyncrasies of an individual platform, is interesting.

Additionally, power consumption is an interesting proxy for joint state. I did not use feedback servos, so the agent must assume that all actions get executed within some fixed window after they are issued. If the arm enters a bad configuration, e.g. hitting an immovable object, power consumption will spike as the servo controller applies more torque. Penalizing current draw indirectly discourages dangerous configurations.

The agent must move all of its joints into a target pose, using as little energy as possible. A sample of the instantaneous current usage is taken during each action execution.

\[ r(s,a,s') =  \left\{
\begin{array}{ll}
	50 & \text{if all joint angles are match the target pose} \\
	-current & \text{otherwise} \\
\end{array} 
\right. \]


\subsection{Learning Approaches}

\subsection{Value Function Based?}

The servo controller library used for this project allows joint angles to be specified with single degree precision. The space of possible joint configurations and actions is large enough to be essentially continuous. In my previous project, I coarse coded the states and restricted the action set to make the problem fit within the system's memory constraints, but this made it impossible for the agent to represent the optimal policy. If we would like to achieve better than minimally-functional policies, we need to use a more flexible representation, but the scale of the problem means that .

\subsection{Policy Gradient}

For this task, policies are much easier to model and optimize over than value functions. This suggests that a policy gradient approach may be appropriate. The book provides one possible policy parameterization for a continuous state-action problem,

\begin{gather*}
	\pi (a | s, \thetab) =  \frac{1}{\sigma(s, \thetab)\sqrt{2\pi}}\exp\bigg({-\frac{(a - \mu(s, {\thetab}))^2}{2\sigma(s, \thetab)})}\bigg)\\
	\thetab = [\thetab^\mu, \thetab^\sigma] \\
	\mu(s, \thetab) = \thetab^{\mu\top}\phib\\
	\sigma(s, \thetab) = \exp(\thetab^{\sigma\top}\phib)\\
	\exp(x) = e^x\\
\end{gather*}

where $a$ is a scalar real-valued action. Because the agent has to pick six joint movements at each time step, this representation can simply be instantiated once for each joint, and each weight matrix can be learned independently. This is not ideal because the distribution of movements for a single joint should be influenced by the distributions of the other joints. A better policy representation might be the multivariate normal distribution,

\begin{gather*}
	\pi (\bm{a} | s, \thetab) =  \frac{1}{\sqrt{(2\pi)^k|\bm{\Sigma}(s, \thetab)|}} \exp\bigg(-(\bm{a} - \bm{\mu}(s, {\thetab}))^\top \bm{\Sigma}(s, \thetab)^{-1}{(\bm{x} - \bm{\mu}(s, \thetab))}\bigg)\\
		\bm{\Sigma}(s, \thetab) =\quad?\\
		\bm{\mu}(s, {\thetab}) =\quad?\\
\end{gather*}
where $\bm{\Sigma}$ is the covariance matrix and $|\bm{\Sigma}|$ is its determinant. However, it is not clear how best to parameterize a representation of $\bm{\mu}$ and $\bm{\Sigma}$, and the derivation of its gradient is more complicated. For this project, I will use six independent normal policies.


\subsubsection{Function Approximation}

Approximation allows tight control of the amount of memory being used to represent the value function. Because of the complex update step however, only at most half of the memory can be used for storing weights. Even then, the update step must be implemented carefully to control the stack size. Consider the episodic semi-gradient one-step Sarsa update:

\begin{equation}\label{eqn:update}
\thetab_{t+1} = 
\thetab_t + 
\alpha \Big[
	R_{t+1} + 
	\gamma \hat{q}(S_{t+1}, A_{t+1}\, \thetab_t) 
	- \hat{q}(S_t, A_t, \thetab_t)
\Big]
\nabla\hat{q}(S_t, A_t, \thetab_t)\tag{1}
\end{equation}

And in the linear case:

\begin{equation}\label{eqn:linear_update}
	\thetab_{t+1} = 
	\thetab_t + 
		\alpha \Big[
			R_{t+1} + \gamma\, \thetab_t^\top\phib_{t+1} \space
			- \thetab_t^\top\bm{\phib}_t
		\Big]\bm{\phib}_t\tag{2}
\end{equation}

It is possible to implement the update using only $n$ additional space, where $n$ is the number of weights, but this is easy to do incorrectly. If the action selection step is placed after the memory allocation, the stack will consume $2n$ memory; maximizing the value function over possible next states requires an additional $n$ stack space.

	\begin{algorithm}
		\caption{Memory-conscious Episodic Semi-gradient One-step Sarsa}
		\label{alg:update}
		\begin{algorithmic}[1] % The number tells where the line numbering should start
			\Procedure{Update}{$S_t$, $A_t$, $S_{t+1}$, \bm{$\theta$}}
			\State $A_{t+1} \gets $ choose action from $S_{t+1}$ according to policy
			\State Allocate \bm{$x$} to be a vector the size of \bm{$\theta$}, and floats $r$ and $a$
			\State $r\gets r(S_t$, $A_t$, $S_{t+1}$)
			\State $\bm{x} \gets \phib(S_{t+1}, A_{t+1})$ \Comment store $\phib_{t+1}$
			\State $a \gets \thetab^\top \bm{x}$ \Comment calculate $v(S_{t+1},A_{t+1})$ so we can discard $\phib_{t+1}$
			\State $\bm{x} \gets \phib(S_{t}, A_{t})$ \Comment store $\phib_t$
			\State $a \gets r + \gamma v - \thetab^\top \bm{x}$ \Comment $a$ is now the bracketed term in eq. $\ref{eqn:update}$
			\State $\bm{x} \gets (\alpha \space a)\bm{x}$ \Comment $\bm{x}$ is now the weight update
			\State \bm{$\theta} \gets \thetab + x$
			\
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}
	
An approximation approach implemented on the microcontroller can use at most 1kb of RAM, or 250 features (less the incidental stack space required during the update step). It was not clear that this would be sufficient for good performance, but it was the only feasible approach. I implemented a semi-gradient one-step Sarsa agent using a linear function approximator.
	
I have not dwelt on time efficiency since, even with software floating point operations, 16MHz permits a fair amount of computation. For this project, I was satisfied as long as actions could be selected more quickly than the servos could execute them. Meeting this deadline, which was about 100ms, or 1.6 million cycles, was not an issue, even while the device was also streaming logging information over serial. If time performance requirements were tighter, special attention would need to be paid to the action selection process, which involves $|A|$ value function queries, each costing $n$ multiplications
	
	

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Experimental Setup}


The agent learns for 50 episodes using $\alpha =$ 0.2, $\gamma =$ 0.99,  following an $\epsilon$-greedy policy with $\epsilon=$ 0.1. Then, the agent operates in an evaluation mode for 50 episodes with $\alpha=\epsilon=0.0$. During this period, episodes are limited to 200 steps in case the agent executes a policy that never reaches the goal. A small delay is used between action execution and sensing to allow the arm to settle. The photocell threshold is calibrated before every session to prevent spurious activations.

\subsection{Features}

I used a simple discretization of the state space. The range of each joint was divided into 8\space\space20\degree\space sections. The two sets of section features along with a binary feature for the LED state were then gridded, resulting in 128 mutually exclusive binary features. 

Because of the large number of actions, it was not feasible to maintain separate approximators per action, so I used three action features: two characterizing the direction of each joint's movement, taking values in $\{0, 1, -1\}$, and one binary feature describing whether or not the LED is activated by the action. Clearly, three features is insufficient to encode the differences in value for all actions across the state space, however there was not enough memory to grid these features with the state features. The total number of weights in the representation is still below the theoretical maximum number of weights for the microcontroller, so additional features could have been added with more time.


\subsection{Actions}

At each time step, the agent selects one entry from each column to form its action. If the LED is already on and the agent chooses an action that includes turning the LED on, the light remains activated for the next timestep. The behavior is symmetric in the LED off case.

\begin{center}
	\begin{tabular}{ l l l}
		Joint 1 & Joint 2 & LED\\ \midrule
		Move left & Move left & Turn on\\
		Move right & Move right & Turn off\\
		No movement & No movement & 
		
	\end{tabular}
\end{center}

To better support the state representation, I set the agent's movement increment to 20\degree. This ensures that the state resulting from any movement has a different feature vector than the previous state. This change does limit the granularity of the agent's movements, but without it, the agent must randomly escape the 20\degree\space sections by repeatedly making smaller movements, which makes learning take much longer. 

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------


\section{Results}


	\begin{figure}[h]
		\begin{center}
			%\includegraphics[width=\textwidth]{figure_0.pdf}
			\caption{The agent's performance. The first 50 episodes are learning time, the rest are evaluations. Due to the time expense of collecting data from the robot, I could only run ten trials. Still, within reasonable confidences, the agent's evaluation averages a reward of 44, demonstrating that it was able to quickly point the LED towards the photocell from arbitrary start positions.}
		\end{center}
	\end{figure}
	
\subsection{Discussion}

The agent learns and generalizes a fairly good policy within its first fifty steps. The evaluation period demonstrates that the learned policy performs well from arbitrary start positions. 

Even though it points the LED at the photocell consistently, the agent does not learn the LED reward dynamics. As can be seen in the video\footnote{https://youtu.be/SCv1AomFDG0}, it opts to leave the light on at all times. This is not unexpected, since it lacks features that describe the interaction of the joint position with the value of actions that turn the LED on. The weight associated with the LED activation action-feature is forced to represent the value of LED actions from any state. Averaged across the entire state space, turning the light on has a higher value than turning it off, so it always leaves it on. Efficient LED activation is not as important as joint movement, so it seems reasonable to prioritize detailed state-space representation over features that would better capture the use of the light.


%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Conclusions}

I have demonstrated that it is possible to implement an embedded agent that achieves good performance on an arm control task, even with extraordinary memory constraints. Further work could investigate more sophisticated features, or explore the performance and memory characteristics of policy gradient methods in the same domain.

\clearpage

\/*
\section{Appendix A: Images}

	\begin{figure}[!htb]
		\centering
		\includegraphics[width=10cm]{../photos/.jpg}
		\label{fig:}
	\end{figure}
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=10cm]{../photos/.jpg}
		\caption{}
		\label{fig:}
	\end{figure}
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=10cm]{../photos/.jpg}
		\caption{}
		\label{fig:}
	\end{figure}
*/

\section{Appendix B: Bill of Materials}


\begin{center}
	\begin{tabular}{ l c c  p{5cm} }
		\toprule
		Component & Quantity & Unit Price (\$) & Note \\ \midrule
		Teensy 3.2 & 1 & 18.00 &  \\ 
		MTR955R servo & 6 & 4.00 & Any standard servo.\\ 
		Arm kit & 1 & 30.00 & \\
		ASC112 current sensor & 1 & 4.00 & Or similar\\
		5v 5.0A power supply & 1 & 16.00 & If variable supply unavailable. \\ 
		Breadboard & 1 & 4.00 & \\
		Assorted jumpers & & 3.00 & \\
		Adhesives, project surface & & 4.00 & \\
		\bottomrule
		
	\end{tabular}
\end{center}
	
\section{Appendix C: Derivation of the Gradient}

The book does not provide the gradient for the parameterized normal policy and I was not able to quickly find it elsewhere. Its derivation is included as an exercise. $\phi(s)$ is abbreviated $\phib$
\begin{gather*}
\pi (a | s, \thetab) =  \frac{1}{\sigma(s, \thetab)\sqrt{2\pi}}\exp\bigg({-\frac{(a - \mu(s, {\thetab}))^2}{2\sigma(s, \thetab)})}\bigg)\\
\thetab = [\thetab^\mu, \thetab^\sigma] \\
\mu(s, \thetab) = \thetab^{\mu\top}\phib\\
\sigma(s, \thetab) = \exp(\thetab^{\sigma\top}\phib)\\
\exp(x) = e^x\\
\end{gather*}
\begin{align*}
\grad{\thetab} \log\pi (a | s, \thetab) &= \grad{\thetab} \log \bigg[ \frac{1}{\exp(\thetab^
	{\sigma\top}\phib)\sqrt{2\pi}}\exp\bigg(- \frac{(a - \thetab^{\mu\top}\phib)^2}{2\exp(\thetab^{\sigma\top}\phib)}\bigg)\bigg]\\
&= \grad{\thetab}\log[\text{AB}] \\
&= \big[\grad{\thetab^\mu} \log[\text{AB}],\quad\grad{\thetab^\sigma}\log[\text{AB}]\big]  \quad\text{(by definition of theta)}\\
&= \big[\grad{\thetab^\mu}[\log\text{A}]+ \grad{\thetab^\mu}[\log\text{B}],\quad \grad{\thetab^\sigma}[\log\text{A}]+ \grad{\thetab^\sigma}[\log\text{B}]\big]  \quad\text{(by log rules)}\\
&= \bigg[\frac{{-\phib} (a - \thetab^{\mu\top}\phib)^2}{2\exp(\thetab^{\sigma\top}\phib)}  ,\quad {-\phib} + \frac{{\phib} (a - \thetab^{\mu\top}\phib)^2}{2\exp(\thetab^{\sigma\top}\phib)} \bigg] \\
 \\
\end{align*}
\subsection{Gradient with respect to $\bm{\theta}^\mu$}
\begin{align*}
\grad{\thetab^\mu} \log\text{A} &= \grad{\thetab^\mu} \log \bigg[\frac{1}{\exp(\thetab^
	{\sigma\top}\phib)\sqrt{2\pi}}\bigg]\\
&=  0\\
\end{align*}
\begin{align*}
\grad{\thetab^\mu} \log \text{B} &= \grad{\thetab^\mu} \log \bigg[ \exp\bigg(- \frac{(a - \thetab^{\mu\top}\phib)^2}{2\exp(\thetab^{\sigma\top}\phib)}\bigg)\bigg]\\
&= \grad{\thetab^\mu} \bigg[-\frac{1}{2} (a - \thetab^{\mu\top}\phib)^2\exp(\thetab^{\sigma\top}\phib)^{-1}\bigg]\\
&= \grad{\thetab^\mu} \bigg[-\frac{1}{2} (a - \thetab^{\mu\top}\phib)^2\bigg] \exp(\thetab^{\sigma\top}\phib)^{-1} + -\frac{1}{2} (a - \thetab^{\mu\top}\phib)^2 \grad{\thetab^\mu} \bigg[\exp(\thetab^{\sigma\top}\phib)^{-1}\bigg]\\
&= \grad{\thetab^\mu} \bigg[-\frac{1}{2} (a - \thetab^{\mu\top}\phib)^2\bigg] \exp(\thetab^{\sigma\top}\phib)^{-1} + \quad 0\\
&= -(a - \thetab^{\mu\top}\phib)\grad{\thetab^\mu} \bigg[a - \thetab^{\mu\top}\phib\bigg] \exp(\thetab^{\sigma\top}\phib)^{-1} \\
&= \frac{{\phib}(a - \thetab^{\mu\top}\phib) }{\exp(\thetab^{\sigma\top}\phib)}\\
\end{align*}
\clearpage
\subsection{Gradient with respect to $\bm{\theta}^\sigma$}
\begin{align*}
	\grad{\thetab^\sigma} \log \text{A} &= \grad{\thetab^\sigma} \log \bigg[\frac{1}{\exp(\thetab^
		{\sigma\top}\phib)\sqrt{2\pi}}\bigg]\\
&= \grad{\thetab^\sigma} \log \bigg[\exp(\thetab^{\sigma\top}\phib)^{-1} \frac{1}{\sqrt{2\pi}}\bigg]\\
&= \grad{\thetab^\sigma} \bigg[-(\thetab^{\sigma\top}\phib) \bigg] + \grad{\thetab^\sigma} \bigg[ \log \frac{1}{\sqrt{2\pi}}\bigg]\\
&=  {-\phib}\\
\end{align*}

\begin{align*}
\grad{\thetab^\sigma} \log \text{B} &= \grad{\thetab^\sigma} \log \bigg[ \exp\bigg(- \frac{(a - \thetab^{\mu\top}\phib)^2}{2\exp(\thetab^{\sigma\top}\phib)}\bigg)\bigg]\\
&=\grad{\thetab^\sigma} \bigg[ - \frac{1}{2}(a - \thetab^{\mu\top}\phib)^2\exp(\thetab^{\sigma\top}\phib)^{-1}\bigg]\\
&=\grad{\thetab^\sigma} \bigg[ - \frac{1}{2}(a - \thetab^{\mu\top}\phib)^2\bigg]\exp(\thetab^{\sigma\top}\phib)^{-1} +  - \frac{1}{2}(a - \thetab^{\mu\top}\phib)^2 \grad{\thetab^\sigma} \bigg[\exp(\thetab^{\sigma\top}\phib)^{-1}\bigg]\\
&=0\quad +\quad  - \frac{1}{2}(a - \thetab^{\mu\top}\phib)^2 \grad{\thetab^\sigma} \bigg[\exp(\thetab^{\sigma\top}\phib)^{-1}\bigg]\\
&= \frac{{\phib} (a - \thetab^{\mu\top}\phib)^2}{2\exp(\thetab^{\sigma\top}\phib)}\\
\end{align*}




	
\end{document}
